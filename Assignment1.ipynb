{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amritamenon/UTS_ML2019_ID12903264/blob/master/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSKLZ3d6tcPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXHK_ic7IIHI",
        "colab_type": "text"
      },
      "source": [
        "Introduction\n",
        "\n",
        "\n",
        "The article explores the efficiency of the Nearest Neighbor Search problem and how effectively it answers queries for finding the closest point, in a set of points. The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement machine learning algorithm that is utilised to solve both classification and regression problems. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s879vP3aIIFp",
        "colab_type": "text"
      },
      "source": [
        "Content\n",
        "\n",
        "The Nearest Neighbor Search problem efficiently answers queries for finding the closest point, in a set of points. Many efficient solutions have been discovered for the case when points lie in a space of constant dimensions and the problem can be solved in O(log⁡n) time, with only O(n) storage. With large-dimensional sets, the solution becomes progressively less efficient, with O(d^(o(1))  log⁡〖n)〗  query time and n^(O(d)) space requirement. The authors postulate that despite decades of effort, the best algorithms are\n",
        "- algorithms that are low in preprocessing cost but suffer linear time increase as count and distance function increases\n",
        "- algorithms that don’t suffer linear processing time increase with count and distance function but have exponentially growing processing cost\n",
        "and overall still devolve into a brute force approach which compares every point to every other, thereby exponentially increasing storage, processing and time complexities and refer to this as the ‘curse of dimensionality’. The problem with these algorithms also extends to how the points are stored and various data structures have been used to model the data, such as k-d trees, R-trees and structures based on space-filling curves but all these perform well only in low dimensional space. The repeat lack of success in removing exponential dependence has led researchers to conjecture that no efficient solutions can exists when the dimensions are large enough and led to the question of whether allowing answers to be approximate, can remove the dependence on dimensionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ml7PZQIIEM",
        "colab_type": "text"
      },
      "source": [
        "Innovation\n",
        "\n",
        "The lack of success in solving the dimensionality issue led to the question of whether approximation could remove the dependence. Specifically, find the c-approximate nearest neighbors (c-NNS) and instead of reporting point q closest to p, report all q that is with c times the expected distance. The authors, in this paper provide results for approximate nearest neighbors and address the issue by first reducing c-NNS to a new problem, point location in equal balls (PLEB). For efficient storage and retrieval, they use a novel data structure called ring-cover trees. This technique now allows an optimization problem of finding a nearest neighbor, in a large dimension space, to be converted into a more understood problem of creating a parameterized search function.\n",
        "Through this paper, the authors offer two algorithms. One algorithm (bucketing method) where a bounded number of points are composed into a ball so that searching them bounds the problem to O(d) though preprocessing time varies by distance function and another algorithm (locality-sensitivity hashing) that predicts that probability of collision is higher for points closer by than far apart. These algorithms and the associated data structure allow for data to be inserted and deleted in real time, reducing preprocessing time and complexity. \n",
        "The authors further point out how these algorithms can be used in a variety of real world problems like data compression, data mining, clustering web documents and retrieving matching histograms from a large set of samples. In addition, the concept of locality-sensitive hashing has independent use in applications involving information retrieval, pattern recognition, dynamic closest-pairs and fast clustering.\n",
        "While the authors have created interesting simplifications to the problem and have called out potential uses of this, there are no concrete examples of how this could actually be used. However, from additional reading, it appears that locality-sensitive hashing has found use in problems around computational biology, computer vision, drug design etc. which require searching in high dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V85K3IyWIIAY",
        "colab_type": "text"
      },
      "source": [
        "Technical Quality\n",
        "\n",
        "The authors provide several results for the approximate nearest neighbors. Reducing time and operational complexity starts with efficient insertion and deletion of points and the authors provide different algorithms based on both a deterministic and random data structure, that optimizes differently on processing time and space. They further postulate which of the algorithms will be better suited for high dimensionality and also show how the ANN solutions can help with other proximity problems such as Minimum Spanning trees.\n",
        "The actual technique of solving for nearest neighbors involves two steps – (1) creating a ‘decision version’ of the ANN by building two different data structures and (2) reducing the ANN problem to just near neighbor problem. The authors acknowledge numerous algorithms that suffer from the curse of dimensionality and show how they helped inspire the algorithms proposed in this paper. Overall the authors created 5 propositions that laid out the premise of their theory. They also introduced 9 definitions that covered aspects from an approximation of the problem to an approximation for the data structures and proved the propositions through 5 theorems and 6 corollaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VBthsA0IVuY",
        "colab_type": "text"
      },
      "source": [
        "Application and X-factor\n",
        "\n",
        "The proposed application domain for c-NNS is to find the closest matching point, to a given query, in a set of large dimensional points. Very specific uses for this could be finding missing pixel(s) in a large picture based on location where it is expected to be, finding a specific document in a large set, allowing a computer to navigate with vision like a human would and so on. In broader terms, efficient solutions for approximate nearest neighbor finds use in databases (content based image retrieval), internet marketing (context aware advertisements), document editing (getting spell checking recommendations), chemical similarity (finding drugs that match) and many more such practical problems.\n",
        "Since querying for matches in a large set is a common problem, this classification mechanism has already found wide spread use.  However from general reading, I believe c-NNS could find use in these problems\n",
        "\tHelping a delivery company create a cluster of nearby addresses so that delivery routes can be optimized to minimize driving distance\n",
        "\tDoing face recognition and identifying the closest match in a large set of photographs\n",
        "Overall the fact that the authors were able to simplify a long confounding problem is noteworthy. While solutions did exist, reducing the time and storage complexity appears to have enabled computers to step in and greatly simplify the problem. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJgR9NckI0R_",
        "colab_type": "text"
      },
      "source": [
        "Presentation\n",
        "\n",
        "The authors have presented this paper with a mathematical bent of mind. The problem has been decomposed into an approximation of the main problem and explained through use of theorems and proofs where required. While the explanations were relative straightforward, the advanced set theory mathematics of this solution is very involved and this paper is not for all consumers. This paper has been republished since original writing and while the authors refer to potential uses, no actual use case for this approximation algorithm has been picked and handled in detail, to help the reader understand better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQFTkC9zlkoo",
        "colab_type": "text"
      },
      "source": [
        "References \n",
        "\n",
        "Medium. (2019). Machine Learning Basics with the K-Nearest Neighbors Algorithm. [online] Available at: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761 [Accessed 28 Aug. 2019].\n",
        "\n"
      ]
    }
  ]
}